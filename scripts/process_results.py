#!/usr/bin/env python3
"""
Processes the results JSON file generated by run_evaluation.py to compute
meta-statistics and prepare data for a dashboard display.
"""

import json
import os
import logging
import argparse
import statistics
from collections import defaultdict
import sys
import numpy as np # Import numpy

# --- Add project root to sys.path --- Start ---
# Get the absolute path of the script's directory
script_dir = os.path.dirname(os.path.abspath(__file__))
# Get the absolute path of the project root (assuming scripts is one level below root)
project_root = os.path.dirname(script_dir)
# Add the project root to sys.path
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print(f"Added {project_root} to sys.path")
# --- Add project root to sys.path --- End ---

try:
    from scripts.config_loader import get_config, Config
    from scripts.logger_setup import setup_logger
except ImportError as e:
    print(f"Error importing modules: {e}")
    print("Please ensure the script is run from the project root or the PYTHONPATH is set correctly.")
    sys.exit(1)

logger = logging.getLogger(__name__) # Use standard logger

def process_individual_result(result_entry, config: Config):
    """Processes a single entry from the results.json list."""
    processed = result_entry.copy() # Start with original data

    # --- Get Thresholds ---
    similarity_threshold = config.get(
        'geometry_check.similarity_threshold_mm',
        0.5 # Replaced DEFAULT_CONFIG lookup with hardcoded value
    )
    hausdorff_threshold = config.get(
         'geometry_check.hausdorff_threshold_mm',
         1.0 # Replaced DEFAULT_CONFIG lookup with hardcoded value
     )
    volume_threshold_percent = config.get(
         'geometry_check.volume_threshold_percent',
         1.0 # Replaced DEFAULT_CONFIG lookup with hardcoded value
     )

    # --- Extract Flags and Statuses ---
    scad_generation_success = bool(result_entry.get("output_scad_path")) and result_entry.get("generation_error") is None
    processed['scad_generation_success'] = scad_generation_success

    checks_dict = result_entry.get("checks", {}) # Get checks sub-dict or empty
    processed['individual_check_statuses'] = checks_dict # Store raw checks

    # Did geometry checks run reasonably well (i.e., produced a checks dict)?
    # Consider it "ran" if the 'checks' dictionary exists and is not empty.
    render_check_passed = checks_dict.get("check_render_successful") # Get render status first
    processed['render_success'] = render_check_passed is True # Store render success flag

    # Change: Now requires render_success AND a non-empty checks dictionary.
    geometry_check_run_success = (processed['render_success'] and
                                  isinstance(checks_dict, dict) and
                                  bool(checks_dict))
    processed['geometry_check_run_success'] = geometry_check_run_success

    # --- Evaluate Individual Checks --- 
    # Extract individual check results, defaulting to None if check didn't run or key missing
    watertight_passed = checks_dict.get("check_is_watertight")
    single_comp_passed = checks_dict.get("check_is_single_component")
    bbox_passed = checks_dict.get("check_bounding_box_accurate")
    volume_passed = checks_dict.get("check_volume_passed")
    hausdorff_passed = checks_dict.get("check_hausdorff_passed")

    # Evaluate if all basic validity checks passed (Render, Watertight, Single Component)
    basic_checks_passed = (render_check_passed is True and
                           watertight_passed is True and
                           single_comp_passed is True)
    
    # Evaluate if all *metric* checks passed (BBox, Volume, Hausdorff)
    # These only count if the basic checks passed first
    metric_checks_passed = (basic_checks_passed and
                            bbox_passed is True and
                            volume_passed is True and
                            hausdorff_passed is True)
    processed['all_geometry_checks_passed'] = metric_checks_passed 

    # Evaluate Chamfer similarity check separately
    chamfer_dist = result_entry.get('geometric_similarity_distance')
    chamfer_passes = False
    # Only evaluate chamfer if checks ran and produced a distance
    if geometry_check_run_success and chamfer_dist is not None and not np.isinf(chamfer_dist):
        try:
            chamfer_passes = float(chamfer_dist) <= similarity_threshold
        except (ValueError, TypeError):
            chamfer_passes = False
    processed['chamfer_check_passed'] = chamfer_passes 

    # --- Overall Pipeline Success --- 
    # Requires: SCAD Gen OK, All metric checks passed (which implies basic checks passed), AND Chamfer similarity passed
    overall_pipeline_success = (scad_generation_success and
                                metric_checks_passed and 
                                chamfer_passes) 
    processed['overall_pipeline_success'] = overall_pipeline_success

    # --- Extract Raw Metrics & Add Formatted Details ---
    hausdorff_dist = result_entry.get('hausdorff_99p_distance')
    processed['hausdorff_99p_distance'] = hausdorff_dist
    processed['hausdorff_99p_distance_detail'] = f"{hausdorff_dist:.4f}" if hausdorff_dist is not None else "N/A"

    processed['reference_volume_mm3'] = result_entry.get('reference_volume_mm3')
    processed['generated_volume_mm3'] = result_entry.get('generated_volume_mm3')
    processed['volume_reference_detail'] = f"{processed['reference_volume_mm3']:.2f}" if processed['reference_volume_mm3'] is not None else "N/A"
    processed['volume_generated_detail'] = f"{processed['generated_volume_mm3']:.2f}" if processed['generated_volume_mm3'] is not None else "N/A"

    processed['reference_bbox_mm'] = result_entry.get('reference_bbox_mm')
    processed['generated_bbox_aligned_mm'] = result_entry.get('generated_bbox_aligned_mm')
    processed['bbox_reference_detail'] = ", ".join(map(lambda x: f"{x:.2f}", processed['reference_bbox_mm'])) if processed['reference_bbox_mm'] else "N/A"
    processed['bbox_generated_aligned_detail'] = ", ".join(map(lambda x: f"{x:.2f}", processed['generated_bbox_aligned_mm'])) if processed['generated_bbox_aligned_mm'] else "N/A"
    
    processed['chamfer_distance_detail'] = f"{chamfer_dist:.4f}" if chamfer_dist is not None else "N/A"
    processed['render_status_detail'] = result_entry.get("render_status", "N/A")
    processed['geometry_check_error_detail'] = result_entry.get("check_error") # Keep original error string if present

    return processed

def calculate_meta_statistics(processed_results):
    """Calculates aggregate statistics grouped by model."""
    stats_by_model = defaultdict(lambda: defaultdict(int))
    # Lists to store values for calculating averages/medians
    similarity_scores_by_model = defaultdict(list) # Chamfer
    hausdorff_scores_by_model = defaultdict(list) # Hausdorff 99p
    volume_diff_percentages_by_model = defaultdict(list) # Volume % diff

    # --- Tally results per model ---
    for result in processed_results:
        model_name = result.get("model_name", "Unknown Model")
        stats = stats_by_model[model_name]
        checks = result.get('individual_check_statuses', {}) # Use the stored raw checks dict

        # Increment total tasks counter
        stats['total_tasks'] += 1
        
        # --- Count Success Stages --- 
        # Use the boolean flags processed by process_individual_result
        if result.get('scad_generation_success') is True: stats['scad_gen_success_count'] += 1
        # Count render success based on the processed flag
        if result.get('render_success') is True: stats['render_success_count'] += 1 
        # Count check run success based on the processed flag
        if result.get('geometry_check_run_success') is True: stats['geo_check_run_success_count'] += 1
        # Count all checks passed based on the processed flag
        if result.get('all_geometry_checks_passed') is True: stats['all_geo_checks_passed_count'] += 1
        # Count overall success based on the processed flag
        if result.get('overall_pipeline_success') is True: stats['overall_pipeline_success_count'] += 1

        # --- Tally Individual Check Passes --- 
        # Only tally if geometry checks ran successfully
        if result.get('geometry_check_run_success') is True:
             if checks.get('check_volume_passed') is True: stats['volume_passed_count'] += 1
             if checks.get('check_hausdorff_passed') is True: stats['hausdorff_passed_count'] += 1
             if checks.get('check_is_watertight') is True: stats['watertight_passed_count'] += 1
             if checks.get('check_is_single_component') is True: stats['single_comp_passed_count'] += 1
             if checks.get('check_bounding_box_accurate') is True: stats['bbox_passed_count'] += 1
             # Note: render_success is already counted above based on the processed flag

        # --- Collect Numerical Metrics --- 
        # Only collect if geometry checks ran successfully
        if result.get('geometry_check_run_success') is True:
            # Chamfer
            chamfer = result.get('geometric_similarity_distance')
            if chamfer is not None and not np.isinf(chamfer):
                try: similarity_scores_by_model[model_name].append(float(chamfer))
                except (ValueError, TypeError): pass
            # Hausdorff
            hausdorff = result.get('hausdorff_99p_distance')
            if hausdorff is not None and not np.isinf(hausdorff):
                try: hausdorff_scores_by_model[model_name].append(float(hausdorff))
                except (ValueError, TypeError): pass
            # Volume Diff %
            ref_vol = result.get('reference_volume_mm3')
            gen_vol = result.get('generated_volume_mm3')
            if ref_vol is not None and gen_vol is not None and ref_vol != 0:
                try:
                    vol_diff_pct = abs(float(gen_vol) - float(ref_vol)) / abs(float(ref_vol)) * 100
                    volume_diff_percentages_by_model[model_name].append(vol_diff_pct)
                except (ValueError, TypeError, ZeroDivisionError): pass

    # --- Calculate rates and averages ---
    final_meta_stats = {}
    for model_name, counts in stats_by_model.items():
        total = counts['total_tasks']
        scad_success = counts['scad_gen_success_count']
        render_success = counts['render_success_count']
        check_run_success = counts['geo_check_run_success_count'] 
        all_checks_passed = counts['all_geo_checks_passed_count'] 
        overall_success = counts['overall_pipeline_success_count']
        volume_passed = counts['volume_passed_count']
        hausdorff_passed = counts['hausdorff_passed_count']

        model_stats = counts.copy() # Start with counts

        # --- Rates ---
        model_stats['scad_gen_success_rate'] = (scad_success / total * 100) if total > 0 else 0
        model_stats['render_success_rate_rel'] = (render_success / scad_success * 100) if scad_success > 0 else 0
        model_stats['geo_check_run_success_rate_rel'] = (check_run_success / render_success * 100) if render_success > 0 else 0
        model_stats['all_geo_checks_passed_rate_rel'] = (all_checks_passed / check_run_success * 100) if check_run_success > 0 else 0
        model_stats['volume_check_pass_rate_rel'] = (volume_passed / check_run_success * 100) if check_run_success > 0 else 0
        model_stats['hausdorff_check_pass_rate_rel'] = (hausdorff_passed / check_run_success * 100) if check_run_success > 0 else 0
        model_stats['overall_pipeline_success_rate'] = (overall_success / total * 100) if total > 0 else 0

        # --- Averages / Medians --- 
        # Chamfer
        valid_chamfer = similarity_scores_by_model.get(model_name, [])
        if valid_chamfer:
            model_stats['average_chamfer_distance'] = statistics.mean(valid_chamfer)
            model_stats['median_chamfer_distance'] = statistics.median(valid_chamfer)
            model_stats['stdev_chamfer_distance'] = statistics.stdev(valid_chamfer) if len(valid_chamfer) > 1 else 0
        else: model_stats['average_chamfer_distance'] = model_stats['median_chamfer_distance'] = model_stats['stdev_chamfer_distance'] = None

        # Hausdorff (New)
        valid_hausdorff = hausdorff_scores_by_model.get(model_name, [])
        if valid_hausdorff:
            model_stats['average_hausdorff_99p_distance'] = statistics.mean(valid_hausdorff)
            model_stats['median_hausdorff_99p_distance'] = statistics.median(valid_hausdorff)
            model_stats['stdev_hausdorff_99p_distance'] = statistics.stdev(valid_hausdorff) if len(valid_hausdorff) > 1 else 0
        else: model_stats['average_hausdorff_99p_distance'] = model_stats['median_hausdorff_99p_distance'] = model_stats['stdev_hausdorff_99p_distance'] = None

        # Volume Diff % (New)
        valid_vol_diff = volume_diff_percentages_by_model.get(model_name, [])
        if valid_vol_diff:
             model_stats['average_volume_diff_percent'] = statistics.mean(valid_vol_diff)
             model_stats['median_volume_diff_percent'] = statistics.median(valid_vol_diff)
             model_stats['stdev_volume_diff_percent'] = statistics.stdev(valid_vol_diff) if len(valid_vol_diff) > 1 else 0
        else: model_stats['average_volume_diff_percent'] = model_stats['median_volume_diff_percent'] = model_stats['stdev_volume_diff_percent'] = None

        # Remove old key if it exists from previous runs (unlikely but safe)
        if 'average_similarity_distance' in model_stats:
            model_stats.pop('average_similarity_distance')

        final_meta_stats[model_name] = model_stats

    return final_meta_stats

def save_processed_results(processed_results, meta_statistics, output_dir, run_id):
    """Saves the processed results and meta-statistics to JSON files."""
    # Combine into a single dashboard data structure
    dashboard_data = {
        "run_id": run_id,
        "meta_statistics": meta_statistics,
        "results_by_model": defaultdict(list) # Group processed results by model
    }
    for result in processed_results:
        model_name = result.get("model_name", "Unknown")
        dashboard_data["results_by_model"][model_name].append(result)

    # Convert defaultdict back to regular dict for JSON serialization
    dashboard_data["results_by_model"] = dict(dashboard_data["results_by_model"])

    # Ensure the output directory exists (corrected path)
    # Get the project root directory (assuming this script is in project_root/scripts)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    dashboard_output_dir = os.path.join(project_root, "dashboard") # Correct path relative to project root
    os.makedirs(dashboard_output_dir, exist_ok=True)

    # Define output paths
    dashboard_output_path = os.path.join(dashboard_output_dir, "dashboard_data.json")
    # Optional: Save processed list separately if needed
    # processed_list_path = os.path.join(output_dir, f"{run_id}_processed_results.json")

    try:
        logger.info(f"Saving dashboard data to: {dashboard_output_path}")
        with open(dashboard_output_path, 'w') as f:
            json.dump(dashboard_data, f, indent=4)
        # logger.info(f"Saving processed results list to: {processed_list_path}")
        # with open(processed_list_path, 'w') as f:
        #     json.dump(processed_results, f, indent=4)
    except IOError as e:
        logger.error(f"Failed to save processed results: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(description="Process CadEval results and generate dashboard data.")
    parser.add_argument("results_file", help="Path to the results.json file from run_evaluation.py")
    parser.add_argument("--config", default="config.yaml", help="Path to the configuration file.")
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"], help="Set the logging level.")
    parser.add_argument("--log-file", help="Optional path to a log file.")
    # Add run_id argument if results file doesn't contain it
    parser.add_argument("--run-id", help="Manually specify the run ID if not in results file.")
    args = parser.parse_args()

    # Setup logger - use the central setup
    global logger # Ensure we modify the module-level logger
    logger = setup_logger('process_results', args.log_level, args.log_file)

    logger.info(f"--- Starting Results Processing --- Results File: {args.results_file}")

    # Load configuration
    try:
        config = get_config(args.config)
        logger.info(f"Loaded configuration from {args.config}")
    except FileNotFoundError:
        logger.error(f"Configuration file not found: {args.config}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error loading configuration: {e}")
        sys.exit(1)

    # Load results data
    results_list = None
    loaded_run_id = None
    try:
        with open(args.results_file, 'r') as f:
            results_data = json.load(f)
        
        # --- Handle both list and dictionary formats --- 
        if isinstance(results_data, list):
            results_list = results_data
            logger.info(f"Loaded {len(results_list)} result entries directly from list in {args.results_file}")
        elif isinstance(results_data, dict):
            results_list = results_data.get('results')
            loaded_run_id = results_data.get('run_id') # Attempt to get run_id from old dict format
            if results_list is not None:
                 logger.info(f"Loaded {len(results_list)} result entries from 'results' key in {args.results_file}")
            else:
                 logger.error("Results file is a dictionary but lacks a 'results' key.")
                 sys.exit(1)
        else:
            logger.error(f"Unexpected data type loaded from results file: {type(results_data)}")
            sys.exit(1)
            
    except FileNotFoundError:
        logger.error(f"Results file not found: {args.results_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from results file: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Error loading results file: {e}")
        sys.exit(1)

    # Determine run_id (prefer CLI arg, fallback to loaded ID only if dict format)
    run_id = args.run_id or loaded_run_id or 'unknown_run' 
    if run_id == 'unknown_run' and not args.run_id:
        logger.warning("Could not determine run_id from file (list format) or arguments. Using 'unknown_run'. Provide --run-id argument.")
    
    if not results_list:
        # Error message was already logged if applicable
        logger.error("Failed to extract results list from the JSON file.")
        sys.exit(1)

    # Process each result
    processed_results = []
    for entry in results_list:
        try:
            processed_entry = process_individual_result(entry, config)
            processed_results.append(processed_entry)
        except Exception as e:
            task_id = entry.get("task_id", "UNKNOWN_TASK")
            model_name = entry.get("model_name", "UNKNOWN_MODEL")
            logger.error(f"Error processing result for Task {task_id}, Model {model_name}: {e}", exc_info=True)
            # Optionally append a placeholder or skip the entry

    logger.info(f"Processed {len(processed_results)} entries.")

    # Calculate meta-statistics
    try:
        meta_statistics = calculate_meta_statistics(processed_results)
        logger.info(f"Calculated meta-statistics for {len(meta_statistics)} models.")
        # Optionally print summary
        # logger.info("\n--- Meta Statistics Summary ---")
        # logger.info(json.dumps(meta_statistics, indent=4))
        # logger.info("--- End Summary ---\n")
    except Exception as e:
        logger.error(f"Error calculating meta-statistics: {e}", exc_info=True)
        sys.exit(1)

    # Save processed data
    output_dir = os.path.dirname(args.results_file) # Use dir of input file
    try:
        save_processed_results(processed_results, meta_statistics, output_dir, run_id)
        logger.info(f"Successfully saved dashboard data.")
    except Exception as e:
        logger.error(f"Failed to save processed results: {e}", exc_info=True)
        sys.exit(1)

    logger.info("--- Results Processing Finished ---")

if __name__ == "__main__":
    main()