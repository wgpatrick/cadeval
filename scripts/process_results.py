#!/usr/bin/env python3
"""
Processes the results JSON file generated by run_evaluation.py to compute
meta-statistics and prepare data for a dashboard display.
"""

import json
import os
import logging
import argparse
import statistics
from collections import defaultdict
import sys
import numpy as np # Import numpy
import yaml
import glob # Import glob
from typing import List, Dict, Any, Tuple, Optional

# --- Add project root to sys.path --- Start ---
# Get the absolute path of the script's directory
script_dir = os.path.dirname(os.path.abspath(__file__))
# Get the absolute path of the project root (assuming scripts is one level below root)
project_root = os.path.dirname(script_dir)
# Add the project root to sys.path
if project_root not in sys.path:
    sys.path.insert(0, project_root)
    print(f"Added {project_root} to sys.path")
# --- Add project root to sys.path --- End ---

try:
    from scripts.config_loader import get_config, Config
    from scripts.logger_setup import setup_logger
except ImportError as e:
    print(f"Error importing modules: {e}")
    print("Please ensure the script is run from the project root or the PYTHONPATH is set correctly.")
    sys.exit(1)

logger = logging.getLogger(__name__) # Use standard logger

# --- Helper Function: Load Task Complexities --- Start ---
def load_task_complexities(task_dir: str) -> Dict[str, Optional[int]]:
    """Loads task complexities (manual_operations) from YAML files.

    Args:
        task_dir: The directory containing task YAML files.

    Returns:
        A dictionary mapping task_id to its manual_operations count (int) or None.
    """
    complexity_map = {}
    logger.info(f"Loading task complexities from directory: {task_dir}")
    task_files = glob.glob(os.path.join(task_dir, "task*.yaml"))
    if not task_files:
        logger.warning(f"No task YAML files found in {task_dir}")
        return {}

    for task_file in task_files:
        try:
            with open(task_file, 'r') as f:
                task_data = yaml.safe_load(f)
                if not isinstance(task_data, dict):
                    logger.warning(f"Skipping {task_file}: Expected a dictionary, got {type(task_data)}")
                    continue

                task_id = task_data.get("task_id")
                complexity = task_data.get("manual_operations")

                if task_id is None:
                    logger.warning(f"Skipping {task_file}: Missing 'task_id' key.")
                    continue

                if complexity is None:
                    logger.warning(f"Task '{task_id}' in {task_file}: Missing 'manual_operations' key. Complexity will be None.")
                    complexity_map[str(task_id)] = None
                else:
                    try:
                        complexity_map[str(task_id)] = int(complexity)
                    except (ValueError, TypeError):
                        logger.warning(f"Task '{task_id}' in {task_file}: Invalid value '{complexity}' for 'manual_operations'. Must be an integer. Complexity will be None.")
                        complexity_map[str(task_id)] = None

        except yaml.YAMLError as e:
            logger.error(f"Error parsing YAML file {task_file}: {e}")
        except IOError as e:
            logger.error(f"Error reading file {task_file}: {e}")
        except Exception as e:
            logger.error(f"Unexpected error processing {task_file}: {e}", exc_info=True)

    logger.info(f"Loaded complexities for {len(complexity_map)} tasks.")
    return complexity_map
# --- Helper Function: Load Task Complexities --- End ---

# --- Configuration Loading ---
def load_config(config_path="config.yaml"):
    """Loads configuration from YAML file."""
    try:
        logger.info(f"Initializing configuration using path: {config_path}")
        return Config(config_path) # NEW: Pass path string
    except FileNotFoundError:
        logger.error(f"Configuration file not found: {config_path}")
        raise # Re-raise FileNotFoundError to be caught by Config or caller
    except yaml.YAMLError as e:
        logger.error(f"Error parsing configuration file {config_path}: {e}")
        raise # Re-raise YAMLError
    except Exception as e:
        logger.error(f"Unexpected error initializing configuration: {e}")
        raise # Re-raise other exceptions

# --- Results Loading ---
def load_results(results_path: str) -> List[Dict[str, Any]]:
    """Loads results from a JSON file or directory of JSON files."""
    all_results = []
    if os.path.isdir(results_path):
        logger.info(f"Loading results from directory: {results_path}")
        for filename in os.listdir(results_path):
            if filename.endswith("_summary.json"):
                file_path = os.path.join(results_path, filename)
                try:
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        # If the summary file contains a list, extend; otherwise append
                        if isinstance(data, list):
                            all_results.extend(data)
                        elif isinstance(data, dict):
                            all_results.append(data)
                        else:
                             logger.warning(f"Unexpected data type in {filename}: {type(data)}")
                except json.JSONDecodeError:
                    logger.error(f"Error decoding JSON from file: {file_path}")
                except Exception as e:
                    logger.error(f"Error loading file {file_path}: {e}")
    elif os.path.isfile(results_path) and results_path.endswith(".json"):
        logger.info(f"Loading results directly from file: {results_path}")
        try:
            with open(results_path, 'r') as f:
                data = json.load(f)
                if isinstance(data, list):
                     all_results = data # Assume it's a list of result dicts
                     logger.info(f"Loaded {len(all_results)} result entries directly from list in {results_path}")
                else:
                     logger.error(f"Expected a list of results in {results_path}, found {type(data)}")
                     return [] # Return empty on unexpected format
        except json.JSONDecodeError:
            logger.error(f"Error decoding JSON from file: {results_path}")
            return []
        except Exception as e:
            logger.error(f"Error loading file {results_path}: {e}")
            return []
    else:
        logger.error(f"Invalid results path provided: {results_path}")
        return []

    return all_results

# --- Threshold Helper ---
def get_threshold(config: Config, key: str, default: float) -> float:
    """Safely gets a float threshold from config, logs warning and uses default if missing/invalid."""
    try:
        # Use the get method with nested keys
        value = config.get(key)
        if value is None:
            logger.warning(f"Configuration key '{key}' not found, using default: {default}")
            return default
        return float(value)
    except (ValueError, TypeError) as e:
        logger.warning(f"Invalid value for configuration key '{key}' ({value}), using default: {default}. Error: {e}")
        return default
    except Exception as e:
         logger.error(f"Unexpected error getting threshold for key '{key}': {e}. Using default: {default}.", exc_info=True)
         return default

# --- Data Processing ---
def process_data_for_dashboard(results: List[Dict[str, Any]], config: Config) -> List[Dict[str, Any]]:
    """Processes the raw results data into the format needed for the dashboard."""
    dashboard_data = []

    # --- Threshold reading ---
    bbox_tolerance = get_threshold(config, 'geometry_check.bounding_box_tolerance_mm', 1.0)
    hausdorff_threshold = get_threshold(config, 'geometry_check.hausdorff_threshold_mm', 1.0) # Updated default to match geometry_check
    volume_threshold = get_threshold(config, 'geometry_check.volume_threshold_percent', 1.0)
    chamfer_threshold = get_threshold(config, 'geometry_check.chamfer_threshold_mm', 1.0)

    logger.info(f"Using thresholds for dashboard pass/fail determination:")
    logger.info(f"  - BBox Tolerance: +/- {bbox_tolerance} mm")
    logger.info(f"  - Hausdorff 95p Threshold: {hausdorff_threshold} mm") # Log uses 95p threshold key
    logger.info(f"  - Volume Threshold: {volume_threshold}% diff")
    logger.info(f"  - Chamfer Threshold: {chamfer_threshold} mm")
    # --- End Threshold reading ---

    # Format numbers for display
    def fmt(val, precision=4):
        if val is None:
            return "N/A"
        try:
            f_val = float(val)
            if np.isinf(f_val): return "Inf"
            if np.isnan(f_val): return "NaN"
            return f"{f_val:.{precision}f}"
        except (ValueError, TypeError):
            return str(val) # Return original string if not a number

    for entry in results: # Iterating through raw replicate entries
        checks_data = entry.get("checks", {})
        if not isinstance(checks_data, dict):
            logger.warning(f"Skipping entry due to invalid 'checks' format (expected dict): {entry.get('task_id', 'N/A')}/{entry.get('model_name', 'N/A')}")
            checks_data = {} # Use empty dict to avoid errors below

        provider = entry.get("provider") # Get provider for conditional logic

        # --- Determine Base Success Flags --- Adjust for Zoo --- 
        scad_gen_display_value = None
        if provider == "zoo_cli":
             scad_gen_display_value = "N/A" # Zoo doesn't generate SCAD
        else:
             # Original logic for non-Zoo providers
             scad_gen_success = entry.get("generation_error") is None or entry.get("generation_error") == ""
             scad_gen_display_value = "Pass" if scad_gen_success else "Fail"

        render_ok_display = "N/A" # Default
        if provider == "zoo_cli":
             # For Zoo, "Render OK" means STL was successfully generated/found
             stl_path_exists = entry.get("output_stl_path") is not None and entry.get("output_stl_path") != ""
             render_ok_display = "Pass" if stl_path_exists else "Fail"
        else:
             # Original logic for non-Zoo providers
             render_status = entry.get("render_status", "N/A")
             render_success = render_status == "Success"
             render_ok_display = render_success if render_status != "N/A" else "N/A"

        # Check if geometry checks were run AT ALL.
        # We infer this if *any* of the check keys (excluding render) exist and are not None.
        # This handles cases where geometry_check.py might have exited early.
        # Prioritize boolean check results if they exist.
        check_keys_to_verify = [
             "check_is_watertight", "check_is_single_component",
             "check_bounding_box_accurate", "check_volume_passed",
             "check_hausdorff_passed", "check_chamfer_passed"
        ]
        checks_run_executed = any(checks_data.get(key) is not None for key in check_keys_to_verify)

        # --- Process Individual Check Results for Dashboard ---

        # Render OK (Based on actual render status)
        render_ok_display = render_success if render_status != "N/A" else "N/A"

        # Watertight (Directly from checks data if available)
        watertight_passed = checks_data.get("check_is_watertight")

        # Single Component (Directly from checks data if available)
        single_comp_passed = checks_data.get("check_is_single_component")

        # BBox Accuracy (Directly from checks data if available)
        bbox_passed = checks_data.get("check_bounding_box_accurate")

        # Volume Pass (Directly from checks data if available)
        volume_passed = checks_data.get("check_volume_passed")

        # Hausdorff Pass (Directly from checks data if available)
        hausdorff_passed = checks_data.get("check_hausdorff_passed")

        # Chamfer Pass (Directly from checks data if available)
        chamfer_passed = checks_data.get("check_chamfer_passed")

        # --- Determine Overall Checks Run Status for Dashboard ---
        # Show "Pass" if checks were executed AND none of the executed checks failed.
        # Show "Fail" if checks were executed BUT at least one executed check failed.
        # Show "N/A" if checks were not executed at all (e.g., gen failed, STL missing).
        checks_run_display = "N/A" # Default if not executed
        if checks_run_executed:
             all_executed_checks_passed = True
             for key in check_keys_to_verify:
                 result = checks_data.get(key)
                 if result is False: # Check specifically for False, ignore None
                     all_executed_checks_passed = False
                     break
             checks_run_display = "Pass" if all_executed_checks_passed else "Fail"

        # --- Overall Pass Calculation ---
        # Consider a run successful overall if SCAD gen passed AND
        # (EITHER rendering passed AND all executed checks passed)
        # OR (rendering was N/A AND all executed checks passed)
        # This means for zoo_cli, success depends on scad_gen and checks_run_display == "Pass".
        # For others, it depends on scad_gen, render_ok_display == "Pass", and checks_run_display == "Pass".

        components_passed = [scad_gen_success]
        if render_status != "N/A": # Only include render check if it was applicable
             components_passed.append(render_success)
        if checks_run_executed: # Only include check results if they were run
             # Use the individual booleans from checks_data for overall calc
             components_passed.extend(result for key in check_keys_to_verify if (result := checks_data.get(key)) is not None)

        overall_passed = all(comp is True for comp in components_passed if comp is not None)

        # --- Format data for dashboard row ---
        row_data = {
            "Task ID": entry.get("task_id", "N/A"),
            "Rep ID": entry.get("replicate_id", "N/A"),
            "Prompt": entry.get("prompt_key_used", "N/A"),
            "Provider": provider, # Use provider variable
            "Model Name": entry.get("model_name", "N/A"), # ADD MODEL NAME
            "SCAD Gen": scad_gen_display_value, # Use new display value
            "Render OK": render_ok_display, # Use new display value
            # --- Add File Paths for Visualize Command --- Start ---
            "reference_stl_path": entry.get("reference_stl_path"),
            "output_stl_path": entry.get("output_stl_path"),
            # --- Add File Paths for Visualize Command --- End ---
            "Watertight": "Pass" if watertight_passed is True else "Fail" if watertight_passed is False else "N/A",
            "Single Comp": "Pass" if single_comp_passed is True else "Fail" if single_comp_passed is False else "N/A",
            "BBox Acc.": "Pass" if bbox_passed is True else "Fail" if bbox_passed is False else "N/A",
            "Volume Pass": "Pass" if volume_passed is True else "Fail" if volume_passed is False else "N/A",
            "Hausdorff Pass": "Pass" if hausdorff_passed is True else "Fail" if hausdorff_passed is False else "N/A",
            "Chamfer Pass": "Pass" if chamfer_passed is True else "Fail" if chamfer_passed is False else "N/A",
            "Chamfer (mm)": fmt(entry.get("geometric_similarity_distance")),
            "Hausdorff Dist (95p / 99p mm)": f"95p: {fmt(entry.get('hausdorff_95p_distance'))}\\n99p: {fmt(entry.get('hausdorff_99p_distance'))}",
            "Vol Ref (mm³)": fmt(entry.get("reference_volume_mm3")),
            "Vol Gen (mm³)": fmt(entry.get("generated_volume_mm3")),
            "BBox Ref (mm)": fmt(entry.get("reference_bbox_mm")),
            "BBox Gen Aligned (mm)": fmt(entry.get("generated_bbox_aligned_mm")),
            "Overall Passed": "Pass" if overall_passed else "Fail", # Use calculated boolean
            # Add other fields as needed...
        }
        dashboard_data.append(row_data)

    logger.info(f"Processed {len(dashboard_data)} replicate entries for dashboard.")
    return dashboard_data

# --- New Aggregation Function --- Start ---
def aggregate_replicate_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Aggregates results across replicates for each task/model combination."""
    logger.info(f"Aggregating results across {len(results)} replicate entries...")
    grouped_results = defaultdict(list)
    for entry in results:
        key = (entry.get("task_id"), entry.get("model_name"))
        if key[0] and key[1]: # Ensure both task_id and model_name are present
            grouped_results[key].append(entry)

    aggregated_data = []
    for (task_id, model_name), replicates in grouped_results.items():
        num_replicates = len(replicates)
        agg_entry = {
            "task_id": task_id,
            "model_name": model_name,
            "num_replicates": num_replicates,
            # Aggregate counts
            "gen_success_count": sum(1 for r in replicates if r.get("generation_error") is None),
            "render_success_count": sum(1 for r in replicates if r.get("render_status") == "Success"),
            "checks_run_count": sum(1 for r in replicates if r.get("render_status") == "Success"), # Checks only run if render succeeds
            "watertight_count": sum(1 for r in replicates if r["checks"].get("check_is_watertight") is True),
            "single_comp_count": sum(1 for r in replicates if r["checks"].get("check_is_single_component") is True),
            "bbox_pass_count": sum(1 for r in replicates if r["checks"].get("check_bounding_box_accurate") is True),
            "volume_pass_count": sum(1 for r in replicates if r["checks"].get("check_volume_passed") is True),
            "hausdorff_pass_count": sum(1 for r in replicates if r["checks"].get("check_hausdorff_passed") is True),
            "overall_pass_count": sum(1 for r in replicates if r.get("overall_passed") is True), # Assumes process_data added this
            # Store first entry's descriptive info (assuming it's the same for all reps)
            "task_description": replicates[0].get("task_description"),
            "reference_stl_path": replicates[0].get("reference_stl_path"),
            "llm_config": replicates[0].get("llm_config"),
            # Add lists to store numeric values for stats calculation
            "metrics": defaultdict(list)
        }

        # Collect numeric metrics across replicates where checks ran
        for r in replicates:
            # Only consider metrics if checks were attempted (render successful)
            if r.get("render_status") == "Success":
                # Helper to safely convert to float, return None if invalid/None
                def safe_float(val):
                    if val is None: return None
                    try: return float(val)
                    except (ValueError, TypeError): return None
                
                metrics_to_collect = [
                    "geometric_similarity_distance", "icp_fitness_score", 
                    "hausdorff_95p_distance", "hausdorff_99p_distance",
                    "reference_volume_mm3", "generated_volume_mm3",
                    "render_duration_seconds"
                ]
                for key in metrics_to_collect:
                    val = safe_float(r.get(key))
                    if val is not None:
                        agg_entry["metrics"][key].append(val)
                
                # Calculate volume diff % for stats
                ref_vol = safe_float(r.get("reference_volume_mm3"))
                gen_vol = safe_float(r.get("generated_volume_mm3"))
                if ref_vol is not None and gen_vol is not None and abs(ref_vol) > 1e-6:
                    vol_diff_pct = (abs(gen_vol - ref_vol) / abs(ref_vol)) * 100
                    agg_entry["metrics"]["volume_diff_percent"].append(vol_diff_pct)

        # Calculate statistics (mean, median, stdev) for collected metrics
        for key, values in agg_entry["metrics"].items():
            if values:
                try: agg_entry[f"avg_{key}"] = statistics.mean(values)
                except statistics.StatisticsError: agg_entry[f"avg_{key}"] = None
                try: agg_entry[f"median_{key}"] = statistics.median(values)
                except statistics.StatisticsError: agg_entry[f"median_{key}"] = None
                if len(values) > 1:
                    try: agg_entry[f"stdev_{key}"] = statistics.stdev(values)
                    except statistics.StatisticsError: agg_entry[f"stdev_{key}"] = None
                else:
                    agg_entry[f"stdev_{key}"] = 0.0 # Or None? Stdev is 0 for single point
            else:
                 agg_entry[f"avg_{key}"] = None
                 agg_entry[f"median_{key}"] = None
                 agg_entry[f"stdev_{key}"] = None

        # Remove the raw metrics list
        del agg_entry["metrics"]
        
        aggregated_data.append(agg_entry)

    logger.info(f"Aggregated into {len(aggregated_data)} task/model combinations.")
    return aggregated_data
# --- New Aggregation Function --- End ---

# --- New Summary Statistics Calculation Function --- Start ---
def calculate_summary_statistics(processed_data: List[Dict[str, Any]]) -> Tuple[Dict[str, Dict], Dict[str, Dict]]:
    """Calculates model-level (per prompt) and task-level summary statistics."""
    logger.info("Calculating summary statistics for models (per prompt) and tasks...")
    # Model stats need to be nested by model_name first, then prompt_key
    model_stats = defaultdict(lambda: defaultdict(lambda: {
        "total_replicates": 0,
        "scad_generation_success_count": 0,
        "render_success_count": 0,
        "checks_run_count": 0,
        "overall_pass_count": 0,
        "watertight_pass_count": 0,
        "single_comp_pass_count": 0,
        "bbox_acc_pass_count": 0,
        "volume_pass_count": 0,
        "hausdorff_pass_count": 0,
        "chamfer_pass_count": 0,
        "metrics": defaultdict(list) # Store valid numeric metrics
    }))
    task_stats = defaultdict(lambda: {
        "total_replicates": 0,
        "scad_generation_success_count": 0,
        "render_success_count": 0,
        "checks_run_count": 0,
        "overall_pass_count": 0,
        "watertight_pass_count": 0,
        "single_comp_pass_count": 0,
        "bbox_acc_pass_count": 0,
        "volume_pass_count": 0,
        "hausdorff_pass_count": 0,
        "chamfer_pass_count": 0,
        "metrics": defaultdict(list)
    })

    metrics_to_aggregate = ["chamfer_dist", "haus_95p_dist"] # Add others like "icp_fitness" if needed

    # --- Accumulate Counts and Metrics ---
    for entry in processed_data:
        model_name = entry.get("model_name")
        task_id = entry.get("task_id")
        prompt_key = entry.get("prompt_key", "default") # Get the prompt key used

        if not model_name or not task_id:
            logger.warning(f"Skipping entry with missing model_name or task_id: {entry}")
            continue

        # Get stats dict for the specific model AND prompt
        m_stat = model_stats[model_name][prompt_key]
        t_stat = task_stats[task_id] # Task stats don't need prompt key nesting

        # Increment total counts
        m_stat["total_replicates"] += 1
        t_stat["total_replicates"] += 1

        # Increment success counts based on boolean flags
        if entry.get("scad_gen_success") is True:
            m_stat["scad_generation_success_count"] += 1
            t_stat["scad_generation_success_count"] += 1
        if entry.get("render_success") is True:
            m_stat["render_success_count"] += 1
            t_stat["render_success_count"] += 1
        if entry.get("overall_passed") is True:
            m_stat["overall_pass_count"] += 1
            t_stat["overall_pass_count"] += 1

        # Increment check-related counts only if checks were attempted
        checks_attempted = entry.get("checks_run_executed") is True
        if checks_attempted:
            m_stat["checks_run_count"] += 1
            t_stat["checks_run_count"] += 1
            if entry.get("watertight_passed") is True:
                m_stat["watertight_pass_count"] += 1
                t_stat["watertight_pass_count"] += 1
            if entry.get("single_comp_passed") is True:
                m_stat["single_comp_pass_count"] += 1
                t_stat["single_comp_pass_count"] += 1
            if entry.get("bbox_passed") is True:
                m_stat["bbox_acc_pass_count"] += 1
                t_stat["bbox_acc_pass_count"] += 1
            if entry.get("volume_passed") is True:
                m_stat["volume_pass_count"] += 1
                t_stat["volume_pass_count"] += 1
            if entry.get("hausdorff_passed") is True:
                m_stat["hausdorff_pass_count"] += 1
                t_stat["hausdorff_pass_count"] += 1
            if entry.get("chamfer_passed") is True:
                m_stat["chamfer_pass_count"] += 1
                t_stat["chamfer_pass_count"] += 1

            # Collect numeric metrics if checks were attempted
            # --- Collect Chamfer Distance (using correct key) ---
            chamfer_val = entry.get("geometric_similarity_distance")
            if chamfer_val is not None and not isinstance(chamfer_val, str) and not np.isinf(chamfer_val) and not np.isnan(chamfer_val):
                try:
                    m_stat["metrics"]["chamfer_dist"].append(float(chamfer_val))
                    t_stat["metrics"]["chamfer_dist"].append(float(chamfer_val))
                except (ValueError, TypeError):
                     logger.debug(f"Could not convert Chamfer value '{chamfer_val}' to float.")
            elif isinstance(chamfer_val, str):
                 logger.debug(f"Skipping non-numeric Chamfer value: {chamfer_val}")

            # --- Collect Hausdorff 95p Distance (using correct key) ---
            haus_95p_val = entry.get("hausdorff_95p_distance")
            if haus_95p_val is not None and not isinstance(haus_95p_val, str) and not np.isinf(haus_95p_val) and not np.isnan(haus_95p_val):
                 try:
                    m_stat["metrics"]["haus_95p_dist"].append(float(haus_95p_val))
                    t_stat["metrics"]["haus_95p_dist"].append(float(haus_95p_val))
                 except (ValueError, TypeError):
                     logger.debug(f"Could not convert Hausdorff 95p value '{haus_95p_val}' to float.")
            elif isinstance(haus_95p_val, str):
                 logger.debug(f"Skipping non-numeric Hausdorff 95p value: {haus_95p_val}")

    # --- Calculate Rates and Statistics --- Need to handle nested model_stats
    def calculate_final_stats(stat_dict, is_nested=False):
        final_stats_result = {}
        for primary_key, primary_value in stat_dict.items():
            if is_nested:
                # Process nested dictionary (model -> prompt -> stats)
                nested_final_stats = {}
                for prompt_key, stats in primary_value.items():
                    # Process the individual stat dict (same logic as before)
                    total_reps = stats["total_replicates"]
                    checks_run = stats["checks_run_count"]
                    final = stats.copy()
                    # Calculate rates
                    final["scad_generation_success_rate"] = (stats["scad_generation_success_count"] / total_reps * 100) if total_reps > 0 else 0
                    final["render_success_rate"] = (stats["render_success_count"] / total_reps * 100) if total_reps > 0 else 0
                    final["overall_pass_rate"] = (stats["overall_pass_count"] / total_reps * 100) if total_reps > 0 else 0
                    final["watertight_pass_rate"] = (stats["watertight_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                    final["single_comp_pass_rate"] = (stats["single_comp_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                    final["bbox_acc_pass_rate"] = (stats["bbox_acc_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                    final["volume_pass_rate"] = (stats["volume_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                    final["hausdorff_pass_rate"] = (stats["hausdorff_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                    final["chamfer_pass_rate"] = (stats["chamfer_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                    # Calculate metric statistics
                    for metric, values in stats["metrics"].items():
                        metric_name = metric.replace('_dist', '').replace('haus_','hausdorff_')
                        if values:
                            try: final[f"avg_{metric_name}"] = statistics.mean(values)
                            except statistics.StatisticsError: final[f"avg_{metric_name}"] = None
                            try: final[f"median_{metric_name}"] = statistics.median(values)
                            except statistics.StatisticsError: final[f"median_{metric_name}"] = None
                            if len(values) > 1:
                                try: final[f"stdev_{metric_name}"] = statistics.stdev(values)
                                except statistics.StatisticsError: final[f"stdev_{metric_name}"] = None
                            else:
                                final[f"stdev_{metric_name}"] = 0.0
                        else:
                             final[f"avg_{metric_name}"] = None
                             final[f"median_{metric_name}"] = None
                             final[f"stdev_{metric_name}"] = None
                    del final["metrics"]
                    nested_final_stats[prompt_key] = final
                final_stats_result[primary_key] = nested_final_stats
            else:
                # Process flat dictionary (task -> stats)
                stats = primary_value
                total_reps = stats["total_replicates"]
                checks_run = stats["checks_run_count"]
                final = stats.copy()
                # Calculate rates
                final["scad_generation_success_rate"] = (stats["scad_generation_success_count"] / total_reps * 100) if total_reps > 0 else 0
                final["render_success_rate"] = (stats["render_success_count"] / total_reps * 100) if total_reps > 0 else 0
                final["overall_pass_rate"] = (stats["overall_pass_count"] / total_reps * 100) if total_reps > 0 else 0
                final["watertight_pass_rate"] = (stats["watertight_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                final["single_comp_pass_rate"] = (stats["single_comp_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                final["bbox_acc_pass_rate"] = (stats["bbox_acc_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                final["volume_pass_rate"] = (stats["volume_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                final["hausdorff_pass_rate"] = (stats["hausdorff_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                final["chamfer_pass_rate"] = (stats["chamfer_pass_count"] / checks_run * 100) if checks_run > 0 else 0
                # Calculate metric statistics
                for metric, values in stats["metrics"].items():
                    metric_name = metric.replace('_dist', '').replace('haus_','hausdorff_')
                    if values:
                        try: final[f"avg_{metric_name}"] = statistics.mean(values)
                        except statistics.StatisticsError: final[f"avg_{metric_name}"] = None
                        try: final[f"median_{metric_name}"] = statistics.median(values)
                        except statistics.StatisticsError: final[f"median_{metric_name}"] = None
                        if len(values) > 1:
                            try: final[f"stdev_{metric_name}"] = statistics.stdev(values)
                            except statistics.StatisticsError: final[f"stdev_{metric_name}"] = None
                        else:
                            final[f"stdev_{metric_name}"] = 0.0
                    else:
                         final[f"avg_{metric_name}"] = None
                         final[f"median_{metric_name}"] = None
                         final[f"stdev_{metric_name}"] = None
                del final["metrics"]
                final_stats_result[primary_key] = final
        return final_stats_result

    # Pass is_nested=True for model stats
    final_model_stats = calculate_final_stats(model_stats, is_nested=True)
    final_task_stats = calculate_final_stats(task_stats, is_nested=False)

    logger.info(f"Finished calculating summary statistics for {len(final_model_stats)} models (across prompts) and {len(final_task_stats)} tasks.")
    return final_model_stats, final_task_stats
# --- New Summary Statistics Calculation Function --- End ---

# --- Helper Function: Calculate Complexity Statistics --- Start ---
def calculate_complexity_statistics(processed_data: List[Dict[str, Any]], complexity_map: Dict[str, Optional[int]]) -> Dict[int, Dict]:
    """Calculates statistics grouped by task complexity (manual operations).

    Args:
        processed_data: List of processed replicate results.
        complexity_map: Dictionary mapping task_id to manual_operations count.

    Returns:
        Dictionary keyed by operation count, containing aggregated stats.
    """
    logger.info("Calculating statistics based on task complexity...")
    stats_by_complexity = defaultdict(lambda: {
        "total_replicates": 0,
        "overall_pass_count": 0,
        # Optional: Add lists for other metrics here if needed
        # "metrics": defaultdict(list)
    })

    for entry in processed_data:
        task_id = entry.get("task_id")
        if not task_id:
            continue # Skip if task_id is missing

        operations = complexity_map.get(str(task_id))

        # Only process if complexity is a valid integer
        if operations is not None and isinstance(operations, int):
            stats = stats_by_complexity[operations]
            stats["total_replicates"] += 1
            if entry.get("overall_passed") is True:
                stats["overall_pass_count"] += 1
            # Optional: Collect other metrics here
            # chamfer = entry.get("chamfer_dist")
            # if chamfer not in ["N/A", "Inf", "NaN", None]:
            #     try: stats["metrics"]["chamfer_dist"].append(float(chamfer))
            #     except ValueError: pass # Ignore conversion errors

    # Calculate final rates and averages
    final_complexity_stats = {}
    for op_count, stats in stats_by_complexity.items():
        total_reps = stats["total_replicates"]
        if total_reps > 0:
            avg_pass_rate = (stats["overall_pass_count"] / total_reps * 100)
            final_complexity_stats[op_count] = {
                "total_replicates": total_reps,
                "overall_pass_count": stats["overall_pass_count"],
                "avg_overall_pass_rate": avg_pass_rate
                # Optional: Calculate avg/median/stdev for other collected metrics
            }
        else:
             logger.warning(f"Complexity level {op_count} had 0 total replicates recorded.")

    logger.info(f"Calculated complexity statistics for {len(final_complexity_stats)} complexity levels.")
    # Sort by operation count for predictable output (optional but nice)
    return dict(sorted(final_complexity_stats.items()))

# --- Helper Function: Calculate Complexity Statistics --- End ---

# --- Saving Results ---
def save_dashboard_data(data: List[Dict[str, Any]], output_path: str):
    """Saves the processed data as a JSON file for the dashboard."""
    # Ensure the output directory exists
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"Created dashboard output directory: {output_dir}")
        except OSError as e:
            logger.error(f"Failed to create directory {output_dir}: {e}")
            return # Cannot save if directory cannot be created
            
    json_content = json.dumps(data, indent=4)
    try:
        with open(output_path, 'w') as f:
            f.write(json_content)
        logger.info(f"Dashboard data successfully saved to {output_path}")
    except IOError as e:
        logger.error(f"Error writing dashboard data to {output_path}: {e}")
    except Exception as e:
        logger.error(f"Unexpected error saving dashboard data: {e}")

def main(args):
    # --- Setup ---
    # Initialize logger locally first
    logger = logging.getLogger(__name__)
    try:
        # Setup logger using the function from logger_setup
        # Ensure setup_logger is defined and imported correctly
        # Check if scripts.logger_setup is available
        try:
            from scripts.logger_setup import setup_logger
            setup_logger(__name__, level=args.log_level.upper()) # Use __name__ for the logger
        except ImportError:
            logger.warning("logger_setup not found, using basicConfig.")
            # Fallback to basic logging if logger_setup is missing
            logging.basicConfig(level=args.log_level.upper(), format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            # No need to reassign logger here, basicConfig configures the root logger
            # logger = logging.getLogger(__name__) # REMOVED

        # Load configuration
        config = load_config()
        logger.info("Configuration loaded successfully from config.yaml") # Now logger is guaranteed to exist

        # Load task complexities
        task_dir = config.get('tasks.directory', os.path.join(project_root, 'tasks')) # Get task dir from config or default
        complexity_map = load_task_complexities(task_dir)

        # Load raw results data
        all_results = load_results(args.results_path)
        if not all_results:
            logger.error("No results loaded. Exiting.")
            return

    except (FileNotFoundError, yaml.YAMLError, ImportError, Exception) as e:
         # Catch potential errors during setup
         # Logger will exist here even if basicConfig was used
         logger.critical(f"Critical error during setup: {e}", exc_info=True)
         sys.exit(1) # Exit if setup fails
    # --- End Setup ---

    # --- Prepare Data for Summaries ---
    logger.info("Preparing data for summary calculations...")
    data_for_summaries = []
    for entry in all_results:
        summary_entry = {}
        checks_data = entry.get("checks", {})
        if not isinstance(checks_data, dict): checks_data = {}

        # Copy original keys needed for grouping/identification
        summary_entry["task_id"] = entry.get("task_id")
        summary_entry["model_name"] = entry.get("model_name")
        summary_entry["prompt_key_used"] = entry.get("prompt_key_used", "default")
        summary_entry["provider"] = entry.get("provider") # Keep provider info if needed

        # --- Recalculate boolean flags based on raw data ---
        summary_entry["scad_gen_success"] = entry.get("generation_error") is None or entry.get("generation_error") == ""
        render_status = entry.get("render_status", "N/A")
        summary_entry["render_success"] = render_status == "Success"
        summary_entry["render_applicable"] = render_status != "N/A"
        summary_entry["watertight_passed"] = checks_data.get("check_is_watertight")
        summary_entry["single_comp_passed"] = checks_data.get("check_is_single_component")
        summary_entry["bbox_passed"] = checks_data.get("check_bounding_box_accurate")
        summary_entry["volume_passed"] = checks_data.get("check_volume_passed")
        summary_entry["hausdorff_passed"] = checks_data.get("check_hausdorff_passed")
        summary_entry["chamfer_passed"] = checks_data.get("check_chamfer_passed")
        check_keys_to_verify = [
             "check_is_watertight", "check_is_single_component",
             "check_bounding_box_accurate", "check_volume_passed",
             "check_hausdorff_passed", "check_chamfer_passed"
        ]
        summary_entry["checks_run_executed"] = any(checks_data.get(key) is not None for key in check_keys_to_verify)
        summary_entry["overall_checks_passed"] = False # Default to False
        if summary_entry["checks_run_executed"]:
            all_executed_checks_passed = True
            for key in check_keys_to_verify:
                result = checks_data.get(key)
                if result is False: # Specifically check for False
                    all_executed_checks_passed = False
                    break
            summary_entry["overall_checks_passed"] = all_executed_checks_passed
        components_passed = [summary_entry["scad_gen_success"]]
        if summary_entry["render_applicable"]:
             components_passed.append(summary_entry["render_success"])
        if summary_entry["checks_run_executed"]:
             components_passed.append(summary_entry["overall_checks_passed"])
        summary_entry["overall_passed"] = all(comp is True for comp in components_passed if comp is not None)
        summary_entry["geometric_similarity_distance"] = entry.get("geometric_similarity_distance")
        summary_entry["hausdorff_95p_distance"] = entry.get("hausdorff_95p_distance")

        if summary_entry["task_id"] and summary_entry["model_name"]:
             data_for_summaries.append(summary_entry)
        else:
             logger.warning(f"Skipping entry for summary calculation due to missing task_id or model_name in original data: {entry.get('task_id')}/{entry.get('model_name')}")

    # --- Calculate Summaries ---
    logger.info("Calculating summary statistics for models (per prompt) and tasks...")
    meta_stats, task_stats = calculate_summary_statistics(data_for_summaries)
    logger.info(f"Finished calculating summary statistics for {len(meta_stats)} models (across prompts) and {len(task_stats)} tasks.")

    # --- Calculate Complexity Statistics ---
    logger.info("Calculating statistics based on task complexity...")
    complexity_analysis = calculate_complexity_statistics(data_for_summaries, complexity_map)
    logger.info(f"Calculated complexity statistics for {len(complexity_analysis)} complexity levels.")

    # --- Process Data for Dashboard Detail View ---
    logger.info("Processing data for dashboard detail display...")
    dashboard_details_data = process_data_for_dashboard(all_results, config)

    # --- Restructure data for dashboard JS ---
    logger.info("Restructuring data for the expected dashboard format...")
    # Group dashboard details by model for the final output
    results_by_model_for_dashboard = defaultdict(list)
    for detail_entry in dashboard_details_data:
        # Use "Model Name" as the key for grouping in the dashboard display
        model_name_key = detail_entry.get("Model Name", "Unknown Model")
        results_by_model_for_dashboard[model_name_key].append(detail_entry)

    # Extract run_id
    run_id = "unknown_run"
    try:
        path_part = os.path.basename(args.results_path)
        parent_dir_name = os.path.basename(os.path.dirname(args.results_path))
        if any(char.isdigit() for char in parent_dir_name) and parent_dir_name != 'results':
             run_id = parent_dir_name
        elif os.path.isdir(args.results_path):
             run_id = path_part
        else:
             base_name = os.path.splitext(path_part)[0]
             if base_name.startswith("results_"):
                  run_id = base_name.split("_", 1)[1]
             elif "_" in base_name and any(char.isdigit() for char in base_name.split("_")[0]):
                  run_id = base_name.split("_")[0]
             else:
                  run_id = base_name
        logger.info(f"Extracted run_id: {run_id}")
    except Exception as e:
        logger.warning(f"Could not automatically extract run_id from path '{args.results_path}': {e}")

    # --- Assemble Final Output ---
    final_dashboard_output = {
        "run_id": run_id,
        "meta_statistics": meta_stats,
        "task_statistics": task_stats,
        "complexity_analysis": complexity_analysis,
        "results_by_model": dict(results_by_model_for_dashboard)
    }

    # --- Save Dashboard Data ---
    dashboard_json_path = os.path.join(project_root, "dashboard", "dashboard_data.json")
    logger.info(f"Saving final structured dashboard data to: {dashboard_json_path}")
    save_dashboard_data(final_dashboard_output, dashboard_json_path)

    logger.info("Processing complete.")


if __name__ == "__main__":
    # Argument parsing
    parser = argparse.ArgumentParser(description="Process CadEval results for dashboard and stats.")
    parser.add_argument("results_path", help="Path to the results JSON file or directory.")
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"], help="Set logging level.")

    args = parser.parse_args()

    # Call main execution logic
    main(args)